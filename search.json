[
  {
    "objectID": "agent.html",
    "href": "agent.html",
    "title": "Agent",
    "section": "",
    "text": "Agent (The RL-based language model)\n\nsource\n\n\nAgent\n\n Agent (model:transformers.modeling_utils.PreTrainedModel)\n\nThe RL-based language model.\n\n\n\n\nType\nDetails\n\n\n\n\nmodel\nPreTrainedModel\na pre-trained transformers model\n\n\n\n\nsource\n\n\nAgent.forward\n\n Agent.forward (input_ids:typing.Annotated[torch.Tensor,{'__torchtyping__'\n                :True,'details':('batch_size','seq_len',),'cls_name':'Tens\n                orType'}], attention_mask:Optional[Annotated[torch.Tensor,\n                {'__torchtyping__':True,'details':('batch_size,seq_len',),\n                'cls_name':'TensorType'}]]=None)\n\nsummary\n\n\nAgent Objective\nEquation 2 in the paper https://arxiv.org/abs/2203.02155\n\\(\\begin{aligned} \\operatorname{objective~}(\\phi)= & E_{(x, y) \\sim D_{\\pi_\\phi^{\\mathrm{RL}}}}\\left[r_\\theta(x, y)-\\beta \\log \\left(\\pi_\\phi^{\\mathrm{RL}}(y \\mid x) / \\pi^{\\mathrm{SFT}}(y \\mid x)\\right)\\right]+ \\\\ & \\gamma E_{x \\sim D_{\\text {pretrain }}}\\left[\\log \\left(\\pi_\\phi^{\\mathrm{RL}}(x)\\right)\\right]\\end{aligned}\\)\n\nsource\n\n\nAgentObjective\n\n AgentObjective (model:transformers.modeling_utils.PreTrainedModel,\n                 sft_model:transformers.modeling_utils.PreTrainedModel,\n                 reward_model:Callable, gamma:float, beta:float)\n\nAgent objective.\n\n\n\n\nType\nDetails\n\n\n\n\nmodel\nPreTrainedModel\nthe language model\n\n\nsft_model\nPreTrainedModel\nthe reference model\n\n\nreward_model\ntyping.Callable\nthe reward model\n\n\ngamma\nfloat\n\n\n\nbeta\nfloat\n\n\n\n\n\nsource\n\n\nAgentObjective.forward\n\n AgentObjective.forward (input_ids:typing.Annotated[torch.Tensor,{'__torch\n                         typing__':True,'details':('batch_size','seq_len',\n                         ),'cls_name':'TensorType'}], attention_mask:typin\n                         g.Annotated[torch.Tensor,{'__torchtyping__':True,\n                         'details':('batch_size','seq_len',),'cls_name':'T\n                         ensorType'}])\n\nCalculate the objective value given the input ids and attention mask.\n\n\n\n\n\n\n\n\n\nType\nDetails\n\n\n\n\ninput_ids\ntyping.Annotated[torch.Tensor, {â€˜torchtypingâ€™: True, â€˜detailsâ€™: (â€˜batch_sizeâ€™, â€˜seq_lenâ€™,), â€˜cls_nameâ€™: â€˜TensorTypeâ€™}]\n\n\n\nattention_mask\ntyping.Annotated[torch.Tensor, {â€˜torchtypingâ€™: True, â€˜detailsâ€™: (â€˜batch_sizeâ€™, â€˜seq_lenâ€™,), â€˜cls_nameâ€™: â€˜TensorTypeâ€™}]\n\n\n\nReturns\ntyping.Annotated[torch.Tensor, {â€˜torchtypingâ€™: True, â€˜detailsâ€™: (1,), â€˜cls_nameâ€™: â€˜TensorTypeâ€™}]\nA scalar objective value"
  },
  {
    "objectID": "tutorial_train_reward_model.html",
    "href": "tutorial_train_reward_model.html",
    "title": "How to train a reward model?",
    "section": "",
    "text": "from torch import optim\nfrom torch.utils.data import DataLoader, random_split\n\nimport pytorch_lightning as pl\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nfrom datasets import load_dataset\n\nfrom instruct_goose.reward import RewardModel, PairwiseLoss\nfrom instruct_goose.dataset import PairDataset\n\nStep 1: Create a reward model from a pre-trained language model\n\ntokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\ntokenizer.pad_token = tokenizer.eos_token\n\n\nreward_model = RewardModel(checkpoint=\"gpt2\")\n\nStep 2: Create a Pairwise dataset\n\ndataset = load_dataset(\"CarperAI/openai_summarize_comparisons\", split=\"train\")\ndataset, _ = random_split(dataset, lengths=[10, len(dataset) - 10]) # for demo purposes\n\nUsing custom data configuration CarperAI--openai_summarize_comparisons-79d2c222a15dc8fb\nFound cached dataset parquet (/Users/education/.cache/huggingface/datasets/CarperAI___parquet/CarperAI--openai_summarize_comparisons-79d2c222a15dc8fb/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n\n\n\npair_dataset = PairDataset(dataset, tokenizer)\ndataloader = DataLoader(pair_dataset, batch_size=2)\n\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:00<00:00, 822.85it/s]\n\n\nStep 3: Write a training loop\n\nN_EPOCHS = 1 # for demo purposes\nLEARNING_RATE = 1e-3\n\npairwise_loss = PairwiseLoss()\n\n\nclass LitRewardModel(pl.LightningModule):\n    def __init__(\n        self, model, loss_func, lr\n    ):\n        super().__init__()\n        self.model = model\n        self.loss_func = loss_func\n        self.lr = lr\n    \n    def training_step(self, batch, batch_idx: int):\n        chosen_input_ids, chosen_attention_mask,\\\n        rejected_input_ids, rejected_attention_mask = batch\n        \n        chosen_rewards = self.model(chosen_input_ids, chosen_attention_mask)\n        rejected_rewards = self.model(rejected_input_ids, rejected_attention_mask)\n        \n        loss = self.loss_func(chosen_rewards, rejected_rewards)\n        \n        print(f\"loss={loss}\")\n        \n        return loss\n    \n    def configure_optimizers(self):\n        optimizer = optim.Adam(self.model.parameters(), lr=self.lr)\n        return optimizer\n\n\nlit_model = LitRewardModel(reward_model, pairwise_loss, lr=1e-3)\n\n\ntrainer = pl.Trainer(max_epochs=1, log_every_n_steps=1)\n\nGPU available: False, used: False\nTPU available: False, using: 0 TPU cores\nIPU available: False, using: 0 IPUs\nHPU available: False, using: 0 HPUs\n\n\n\ntrainer.fit(model=lit_model, train_dataloaders=dataloader)\n\nMissing logger folder: /Users/education/DATA/projects/ai/RLHF/instructGOOSE/nbs/lightning_logs\n\n  | Name      | Type         | Params\n-------------------------------------------\n0 | model     | RewardModel  | 124 M \n1 | loss_func | PairwiseLoss | 0     \n-------------------------------------------\n124 M     Trainable params\n0         Non-trainable params\n124 M     Total params\n497.762   Total estimated model params size (MB)\n/Users/education/DATA/projects/ai/RLHF/instructGOOSE/env/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n  rank_zero_warn(\n\n\n\n\n\nloss=-0.2531266510486603\nloss=-0.2498958855867386\nloss=-0.24884334206581116\nloss=-0.2499789297580719\nloss=-0.23997953534126282\n\n\n`Trainer.fit` stopped: `max_epochs=1` reached."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "InstructGoose",
    "section": "",
    "text": "Paper: InstructGPT - Training language models to follow instructions with human feedback"
  },
  {
    "objectID": "index.html#install",
    "href": "index.html#install",
    "title": "InstructGoose",
    "section": "Install",
    "text": "Install\nInstall from PipPy\npip install instruct-goose\nInstall directly from the source code\ngit clone https://github.com/xrsrke/instructGOOSE.git\ncd instructGOOSE\npip install -e .\n\nHow to Train\nFor reward model\nUse ðŸ¤— Accelerate to launch distributed training\naccelerate config\naccelerate launch scripts/train_reward.py"
  },
  {
    "objectID": "index.html#train-the-rl-based-language-model",
    "href": "index.html#train-the-rl-based-language-model",
    "title": "InstructGoose",
    "section": "Train the RL-based language model",
    "text": "Train the RL-based language model\n\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nfrom datasets import load_dataset\n\nimport torch\nfrom torch.utils.data import DataLoader, random_split\nfrom torch import optim\n\nfrom instruct_goose import Agent, RewardModel, RLHFTrainer, RLHFConfig, create_reference_model\n\nStep 1: Load dataset\n\ndataset = load_dataset(\"imdb\", split=\"train\")\ndataset, _ = random_split(dataset, lengths=[10, len(dataset) - 10]) # for demenstration purposes\ntrain_dataloader = DataLoader(dataset, batch_size=4, shuffle=True)\n\nFound cached dataset imdb (/Users/education/.cache/huggingface/datasets/imdb/plain_text/1.0.0/d613c88cf8fa3bab83b4ded3713f1f74830d1100e171db75bbddb80b3345c9c0)\n\n\nStep 2: Load the pre-trained model and tokenizer\n\nmodel_base = AutoModelForCausalLM.from_pretrained(\"gpt2\") # for demonstration purposes\nreward_model = RewardModel(\"gpt2\")\n\ntokenizer = AutoTokenizer.from_pretrained(\"gpt2\", padding_side=\"left\")\neos_token_id = tokenizer.eos_token_id\ntokenizer.pad_token = tokenizer.eos_token\n\nStep 3: Create the RL-based language model agent and the reference model\n\nmodel = Agent(model_base)\nref_model = create_reference_model(model)\n\nStep 4: Train it\n\nmax_new_tokens = 20\ngeneration_kwargs = {\n    \"min_length\":-1,\n    \"top_k\": 0.0,\n    \"top_p\": 1.0,\n    \"do_sample\": True,\n    \"pad_token_id\": tokenizer.eos_token_id,\n    \"max_new_tokens\": max_new_tokens\n}\n\nconfig = RLHFConfig()\nN_EPOCH = 1 # for demonstration purposes\ntrainer = RLHFTrainer(model, ref_model, config)\noptimizer = optim.SGD(model.parameters(), lr=1e-3)\n\n\nfor epoch in range(N_EPOCH):\n    for batch in train_dataloader:\n        inputs = tokenizer(batch[\"text\"], padding=True, truncation=True, return_tensors=\"pt\")\n        response_ids = model.generate(\n            inputs[\"input_ids\"], attention_mask=inputs[\"attention_mask\"],\n            **generation_kwargs\n        )\n        \n        # extract the generated text\n        response_ids = response_ids[:, -max_new_tokens:]\n        response_attention_mask = torch.ones_like(response_ids)\n        \n        # evaluate from the reward model\n        with torch.no_grad():\n            text_input_ids = torch.stack([torch.concat([q, r]) for q, r in zip(inputs[\"input_ids\"], response_ids)], dim=0)\n            rewards = reward_model(text_input_ids)\n        \n        # calculate PPO loss\n        loss = trainer.compute_loss(\n            query_ids=inputs[\"input_ids\"],\n            query_attention_mask=inputs[\"attention_mask\"],\n            response_ids=response_ids,\n            response_attention_mask=response_attention_mask,\n            rewards=rewards\n        )\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        print(f\"loss={loss}\")\n\nloss=-824.6560668945312\nloss=0.030958056449890137\nloss=4.284017562866211"
  },
  {
    "objectID": "index.html#todo",
    "href": "index.html#todo",
    "title": "InstructGoose",
    "section": "TODO",
    "text": "TODO\n\nAdd support custom reward function\nAdd support custom value function\nAdd support non-transformer models\nWrite config class\n\nâœ… Distributed training using ðŸ¤— Accelerate"
  },
  {
    "objectID": "index.html#resources",
    "href": "index.html#resources",
    "title": "InstructGoose",
    "section": "Resources",
    "text": "Resources\nI implemented this using these resources\n\nCopied the load_yaml function from https://github.com/Dahoas/reward-modeling\nHow to build a dataset to train reward model: https://wandb.ai/carperai/summarize_RLHF/reports/Implementing-RLHF-Learning-to-Summarize-with-trlXâ€“VmlldzozMzAwODM2\nHow to add value head in PPO agent: https://github.com/lvwerra/trl\nHow to calculate the loss of PPO agent: https://github.com/lvwerra/trl/blob/main/trl/trainer/ppo_trainer.py\nHow to use PPO to train RLHF agent: https://github.com/voidful/TextRL\nHow PPO works: https://github.com/vwxyzjn/cleanrl/blob/master/cleanrl/ppo.py\nCopied the compute advantages and returns from TLR: https://github.com/lvwerra/trl/blob/d2e8bcf8373726fb92d2110c500f7df6d0bd566d/trl/trainer/ppo_trainer.py#L686"
  },
  {
    "objectID": "utils.html",
    "href": "utils.html",
    "title": "Utility functions",
    "section": "",
    "text": "source\n\nload_yaml\n\n load_yaml (config_path)\n\n\nsource\n\n\nRLHFConfig\n\n RLHFConfig (epsilon:float=0.1, ent_coef:float=0.01, vf_coef:float=0.1)\n\n\n\nReference Model\n\nsource\n\n\ncreate_reference_model\n\n create_reference_model (model)\n\n\n\nConfigs\n\nsource\n\n\nModelConfig\n\n ModelConfig (model_path:str)\n\n\nsource\n\n\nTokenizerConfig\n\n TokenizerConfig (tokenizer_path:str)\n\n\nsource\n\n\nOptimizerConfig\n\n OptimizerConfig (lr:float=0.0001, eps:float=1e-08,\n                  weight_decay:float=1e-06)\n\n\nsource\n\n\nTrainerConfig\n\n TrainerConfig (epochs:int=20)\n\n\nsource\n\n\nPPOConfig\n\n PPOConfig (ent_coef:float=0.01, vf_coef:float=0.5)\n\n\nsource\n\n\nInstructConfig\n\n InstructConfig (model:__main__.ModelConfig,\n                 tokenizer:__main__.TokenizerConfig)"
  },
  {
    "objectID": "reward_model.html",
    "href": "reward_model.html",
    "title": "Reward Model",
    "section": "",
    "text": "Reward Model\n\nsource\n\n\nRewardModel\n\n RewardModel (model_name:str, dropout:float=0.1, device:str='cuda')\n\nReward model.\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nmodel_name\nstr\n\ntransformersâ€™s model name\n\n\ndropout\nfloat\n0.1\n\n\n\ndevice\nstr\ncuda\n\n\n\n\n\nsource\n\n\nRewardModel.forward\n\n RewardModel.forward (input_ids:typing.Annotated[torch.Tensor,{'__torchtyp\n                      ing__':True,'details':('batch_size','seq_len',),'cls\n                      _name':'TensorType'}], attention_mask:typing.Annotat\n                      ed[torch.Tensor,{'__torchtyping__':True,'details':('\n                      batch_size','seq_len',),'cls_name':'TensorType'}]=No\n                      ne)\n\nCalculate reward for each item in a batch.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ninput_ids\ntyping.Annotated[torch.Tensor, {â€˜torchtypingâ€™: True, â€˜detailsâ€™: (â€˜batch_sizeâ€™, â€˜seq_lenâ€™,), â€˜cls_nameâ€™: â€˜TensorTypeâ€™}]\n\n\n\n\nattention_mask\ntyping.Annotated[torch.Tensor, {â€˜torchtypingâ€™: True, â€˜detailsâ€™: (â€˜batch_sizeâ€™, â€˜seq_lenâ€™,), â€˜cls_nameâ€™: â€˜TensorTypeâ€™}]\nNone\n\n\n\nReturns\ntyping.Annotated[torch.Tensor, {â€˜torchtypingâ€™: True, â€˜detailsâ€™: (â€˜batch_sizeâ€™, 1,), â€˜cls_nameâ€™: â€˜TensorTypeâ€™}]\n\nA reward scalar for each item in a batch\n\n\n\n\n\nPairwise Loss\n\\(\\operatorname{loss}(\\theta)=-\\frac{1}{\\left(\\begin{array}{c} K \\\\ 2 \\end{array}\\right)} E_{\\left(x, y_w, y_l\\right) \\sim D}\\left[\\log \\left(\\sigma\\left(r_\\theta\\left(x, y_w\\right)-r_\\theta\\left(x, y_l\\right)\\right)\\right)\\right]\\)\n\nsource\n\n\nPairwiseLoss\n\n PairwiseLoss (*args, **kwargs)\n\nPairwise loss function.\n\nsource\n\n\nPairwiseLoss.forward\n\n PairwiseLoss.forward (chosen_rewards:typing.Annotated[torch.Tensor,{'__to\n                       rchtyping__':True,'details':('batch_size',1,),'cls_\n                       name':'TensorType'}], rejected_rewards:typing.Annot\n                       ated[torch.Tensor,{'__torchtyping__':True,'details'\n                       :('batch_size',1,),'cls_name':'TensorType'}])\n\nCompute the loss value.\n\n\n\n\n\n\n\n\n\nType\nDetails\n\n\n\n\nchosen_rewards\ntyping.Annotated[torch.Tensor, {â€˜torchtypingâ€™: True, â€˜detailsâ€™: (â€˜batch_sizeâ€™, 1,), â€˜cls_nameâ€™: â€˜TensorTypeâ€™}]\nThe reward of the chosen prompt\n\n\nrejected_rewards\ntyping.Annotated[torch.Tensor, {â€˜torchtypingâ€™: True, â€˜detailsâ€™: (â€˜batch_sizeâ€™, 1,), â€˜cls_nameâ€™: â€˜TensorTypeâ€™}]\nThe reward of the rejected prompt\n\n\nReturns\ntyping.Annotated[torch.Tensor, {â€˜torchtypingâ€™: True, â€˜detailsâ€™: (1,), â€˜cls_nameâ€™: â€˜TensorTypeâ€™}]\nA scalar loss"
  },
  {
    "objectID": "trainer.html",
    "href": "trainer.html",
    "title": "Trainer",
    "section": "",
    "text": "\\(L_t^{C L I P+V F+S}(\\theta)=\\hat{\\mathbb{E}}_t\\left[L_t^{C L I P}(\\theta)-c_1 L_t^{V F}(\\theta)+c_2 S\\left[\\pi_\\theta\\right]\\left(s_t\\right)\\right]\\)\n\\(L^{C L I P}(\\theta)=\\hat{\\mathbb{E}}_t\\left[\\min \\left(r_t(\\theta) \\hat{A}_t, \\operatorname{clip}\\left(r_t(\\theta), 1-\\epsilon, 1+\\epsilon\\right) \\hat{A}_t\\right)\\right]\\)\n\\(\\frac{\\pi_\\theta\\left(a_t \\mid s_t\\right)}{\\pi_{\\theta_{\\text {old }}}\\left(a_t \\mid s_t\\right)} = \\log(\\pi_\\theta\\left(a_t \\mid s_t\\right)) - \\log(\\pi_{\\theta_{\\text {old }}}\\left(a_t \\mid s_t\\right))\\)\n\\(r_t(\\theta)=\\frac{\\pi_\\theta\\left(a_t \\mid s_t\\right)}{\\pi_{\\theta_{\\text {old }}}\\left(a_t \\mid s_t\\right)}\\)\n\nsource\n\nRLHFTrainer\n\n RLHFTrainer (model:transformers.modeling_utils.PreTrainedModel,\n              ref_model:transformers.modeling_utils.PreTrainedModel,\n              config:instruct_goose.utils.RLHFConfig)\n\nInitialize self. See help(type(self)) for accurate signature.\n\n\n\n\nType\nDetails\n\n\n\n\nmodel\nPreTrainedModel\nA pre-trained language model\n\n\nref_model\nPreTrainedModel\nA a reference model\n\n\nconfig\nRLHFConfig\n\n\n\n\n\nsource\n\n\nRLHFTrainer.compute_loss\n\n RLHFTrainer.compute_loss (query_ids:typing.Annotated[torch.Tensor,{'__tor\n                           chtyping__':True,'details':('batch_size','seq_l\n                           en',),'cls_name':'TensorType'}], query_attentio\n                           n_mask:typing.Annotated[torch.Tensor,{'__torcht\n                           yping__':True,'details':('batch_size','seq_len'\n                           ,),'cls_name':'TensorType'}], response_ids:typi\n                           ng.Annotated[torch.Tensor,{'__torchtyping__':Tr\n                           ue,'details':('batch_size','seq_len',),'cls_nam\n                           e':'TensorType'}], response_attention_mask:typi\n                           ng.Annotated[torch.Tensor,{'__torchtyping__':Tr\n                           ue,'details':('batch_size','seq_len',),'cls_nam\n                           e':'TensorType'}], rewards:typing.Annotated[tor\n                           ch.Tensor,{'__torchtyping__':True,'details':('b\n                           atch_size',),'cls_name':'TensorType'}])\n\nCalculate PPOâ€™s loss."
  },
  {
    "objectID": "dataset.html",
    "href": "dataset.html",
    "title": "Dataset",
    "section": "",
    "text": "Dataset for Reward Model\n\nsource\n\n\nPairDataset\n\n PairDataset (dataset:Iterable, tokenizer:Callable, max_length:int=1024)\n\nPairwise dataset for train reward model.\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ndataset\ntyping.Iterable\n\nA dataset\n\n\ntokenizer\ntyping.Callable\n\nThe tokenizer of the reward model\n\n\nmax_length\nint\n1024\nMax context length of the reward model\n\n\n\n\n\nDataset for PPO Agent\n\nsource\n\n\nPromptDataset\n\n PromptDataset (dataset:Iterable, tokenizer:Callable, max_length:int=1024)\n\nDataset for train RL-based language model.\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ndataset\ntyping.Iterable\n\nA dataset\n\n\ntokenizer\ntyping.Callable\n\nThe tokenizer of the language model\n\n\nmax_length\nint\n1024\nMax context length of the language model"
  }
]