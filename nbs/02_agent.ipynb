{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agent\n",
    "\n",
    "> RL-based Language model Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from typing import Callable, Tuple, Optional\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Categorical\n",
    "\n",
    "from torchtyping import TensorType\n",
    "from transformers import PreTrainedModel"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agent (The RL-based language model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class Agent(nn.Module):\n",
    "    \"The RL-based language model.\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        model: PreTrainedModel # a pre-trained `transformers` model\n",
    "    ):\n",
    "        super().__init__()\n",
    "        n_embd = model.config.n_embd\n",
    "        self.eos_token_id = model.config.eos_token_id\n",
    "\n",
    "        self.policy_network = model        \n",
    "        self.value_network = nn.Sequential(\n",
    "            nn.Linear(n_embd, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 1),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "    \n",
    "    def get_value(\n",
    "        self, hidden_state: TensorType[\"batch_size\", \"seq_len\", \"n_embd\"]\n",
    "    ) -> TensorType[\"batch_size\", 1]:\n",
    "        \"\"\"Get value from the value network.\"\"\"\n",
    "        return self.value_network(hidden_state)[:, -1, :]\n",
    "\n",
    "    def generate(\n",
    "        self,\n",
    "        input_ids: TensorType[\"batch_size\", \"seq_len\"],\n",
    "        attention_mask: Optional[TensorType[\"batch_size\", \"seq_len\"]] = None,\n",
    "        **kwargs\n",
    "    ) -> TensorType[\"batch_size\", \"seq_len\"]:\n",
    "        output = self.policy_network.generate(\n",
    "            input_ids=input_ids, attention_mask=attention_mask, **kwargs\n",
    "        )\n",
    "        return output\n",
    "    \n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: TensorType[\"batch_size\", \"seq_len\"],\n",
    "        attention_mask: Optional[TensorType[\"batch_size, seq_len\"]] = None\n",
    "    ) -> Tuple[\n",
    "        TensorType[\"batch_size\", \"seq_len\", \"vocab_size\"],\n",
    "        TensorType[\"batch_size\", \"seq_len\", \"vocab_size\"],\n",
    "        TensorType[\"batch_size\", \"seq_len\"],\n",
    "        TensorType[\"batch_size\", 1]\n",
    "    ]: # action, logprobs, entropy, value\n",
    "        \"\"\"_summary_\"\"\"\n",
    "        if attention_mask is None:\n",
    "            base_output = self.policy_network(\n",
    "                input_ids,\n",
    "                output_hidden_states=True,\n",
    "            )\n",
    "        else:\n",
    "            base_output = self.policy_network(\n",
    "                input_ids, attention_mask=attention_mask,\n",
    "                output_hidden_states=True,\n",
    "            )\n",
    "        \n",
    "        last_hidden_state = base_output.hidden_states[-1]\n",
    "        \n",
    "        # takes the logit of the last token\n",
    "        # for each sequence in the batch\n",
    "        logits = base_output.logits[:, -1, :]\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "                                \n",
    "        action_dist = Categorical(probs=probs)\n",
    "        action = action_dist.sample()\n",
    "        entropy = action_dist.entropy()\n",
    "        logprobs = action_dist.log_prob(action)\n",
    "        \n",
    "        # predicted reward value\n",
    "        value = self.get_value(last_hidden_state).squeeze(-1)\n",
    "        \n",
    "        return action, logprobs, entropy, value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/xrsrke/instructGOOSE/blob/main/instruct_goose/agent.py#L18){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### Agent\n",
       "\n",
       ">      Agent (model:transformers.modeling_utils.PreTrainedModel)\n",
       "\n",
       "The RL-based language model.\n",
       "\n",
       "|    | **Type** | **Details** |\n",
       "| -- | -------- | ----------- |\n",
       "| model | PreTrainedModel | a pre-trained `transformers` model |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/xrsrke/instructGOOSE/blob/main/instruct_goose/agent.py#L18){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### Agent\n",
       "\n",
       ">      Agent (model:transformers.modeling_utils.PreTrainedModel)\n",
       "\n",
       "The RL-based language model.\n",
       "\n",
       "|    | **Type** | **Details** |\n",
       "| -- | -------- | ----------- |\n",
       "| model | PreTrainedModel | a pre-trained `transformers` model |"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(Agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/xrsrke/instructGOOSE/blob/main/instruct_goose/agent.py#L55){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### Agent.forward\n",
       "\n",
       ">      Agent.forward (input_ids:typing.Annotated[torch.Tensor,{'__torchtyping__'\n",
       ">                     :True,'details':('batch_size','seq_len',),'cls_name':'Tens\n",
       ">                     orType'}], attention_mask:Optional[Annotated[torch.Tensor,\n",
       ">                     {'__torchtyping__':True,'details':('batch_size,seq_len',),\n",
       ">                     'cls_name':'TensorType'}]]=None)\n",
       "\n",
       "_summary_"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/xrsrke/instructGOOSE/blob/main/instruct_goose/agent.py#L55){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### Agent.forward\n",
       "\n",
       ">      Agent.forward (input_ids:typing.Annotated[torch.Tensor,{'__torchtyping__'\n",
       ">                     :True,'details':('batch_size','seq_len',),'cls_name':'Tens\n",
       ">                     orType'}], attention_mask:Optional[Annotated[torch.Tensor,\n",
       ">                     {'__torchtyping__':True,'details':('batch_size,seq_len',),\n",
       ">                     'cls_name':'TensorType'}]]=None)\n",
       "\n",
       "_summary_"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(Agent.forward)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agent Objective"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Equation 2 in the paper https://arxiv.org/abs/2203.02155"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\begin{aligned} \\operatorname{objective~}(\\phi)= & E_{(x, y) \\sim D_{\\pi_\\phi^{\\mathrm{RL}}}}\\left[r_\\theta(x, y)-\\beta \\log \\left(\\pi_\\phi^{\\mathrm{RL}}(y \\mid x) / \\pi^{\\mathrm{SFT}}(y \\mid x)\\right)\\right]+ \\\\ & \\gamma E_{x \\sim D_{\\text {pretrain }}}\\left[\\log \\left(\\pi_\\phi^{\\mathrm{RL}}(x)\\right)\\right]\\end{aligned}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class AgentObjective(nn.Module):\n",
    "    \"\"\"Agent objective.\"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        model: PreTrainedModel, # the language model\n",
    "        sft_model: PreTrainedModel, # the reference model\n",
    "        reward_model: Callable, # the reward model\n",
    "        gamma: float,\n",
    "        beta: float\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.sft_model = sft_model\n",
    "        self.reward_model = reward_model\n",
    "        self.gamma = gamma\n",
    "        self.beta = beta\n",
    "        \n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: TensorType[\"batch_size\", \"seq_len\"],\n",
    "        attention_mask: TensorType[\"batch_size\", \"seq_len\"]\n",
    "    ) -> TensorType[1]: # A scalar objective value\n",
    "        \"\"\"Calculate the objective value given the input ids and attention mask.\"\"\"\n",
    "        model_logits = self.model(input_ids, attention_mask)\n",
    "        model_dist = F.softmax(model_logits, dim=-1)\n",
    "        \n",
    "        sft_logits = self.sft_model(input_ids, attention_mask)\n",
    "        sft_dist = F.softmax(sft_logits, dim=-1)\n",
    "        \n",
    "        reward_score = self.reward_model(input_ids, attention_mask)\n",
    "        ratio = torch.log(model_dist / sft_dist)\n",
    "        \n",
    "        # compute the coherent of the generated text\n",
    "        coherent = torch.log(model_dist)\n",
    "        objective = (reward_score - self.beta*ratio).mean() + self.gamma * coherent.mean()\n",
    "        return objective"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/xrsrke/instructGOOSE/blob/main/instruct_goose/agent.py#L95){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### AgentObjective\n",
       "\n",
       ">      AgentObjective (model:transformers.modeling_utils.PreTrainedModel,\n",
       ">                      sft_model:transformers.modeling_utils.PreTrainedModel,\n",
       ">                      reward_model:Callable, gamma:float, beta:float)\n",
       "\n",
       "Agent objective.\n",
       "\n",
       "|    | **Type** | **Details** |\n",
       "| -- | -------- | ----------- |\n",
       "| model | PreTrainedModel | the language model |\n",
       "| sft_model | PreTrainedModel | the reference model |\n",
       "| reward_model | Callable | the reward model |\n",
       "| gamma | float |  |\n",
       "| beta | float |  |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/xrsrke/instructGOOSE/blob/main/instruct_goose/agent.py#L95){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### AgentObjective\n",
       "\n",
       ">      AgentObjective (model:transformers.modeling_utils.PreTrainedModel,\n",
       ">                      sft_model:transformers.modeling_utils.PreTrainedModel,\n",
       ">                      reward_model:Callable, gamma:float, beta:float)\n",
       "\n",
       "Agent objective.\n",
       "\n",
       "|    | **Type** | **Details** |\n",
       "| -- | -------- | ----------- |\n",
       "| model | PreTrainedModel | the language model |\n",
       "| sft_model | PreTrainedModel | the reference model |\n",
       "| reward_model | Callable | the reward model |\n",
       "| gamma | float |  |\n",
       "| beta | float |  |"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(AgentObjective)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/xrsrke/instructGOOSE/blob/main/instruct_goose/agent.py#L112){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### AgentObjective.forward\n",
       "\n",
       ">      AgentObjective.forward (input_ids:typing.Annotated[torch.Tensor,{'__torch\n",
       ">                              typing__':True,'details':('batch_size','seq_len',\n",
       ">                              ),'cls_name':'TensorType'}], attention_mask:typin\n",
       ">                              g.Annotated[torch.Tensor,{'__torchtyping__':True,\n",
       ">                              'details':('batch_size','seq_len',),'cls_name':'T\n",
       ">                              ensorType'}])\n",
       "\n",
       "Calculate the objective value given the input ids and attention mask.\n",
       "\n",
       "|    | **Type** | **Details** |\n",
       "| -- | -------- | ----------- |\n",
       "| input_ids | Annotated |  |\n",
       "| attention_mask | Annotated |  |\n",
       "| **Returns** | **Annotated** | **A scalar objective value** |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/xrsrke/instructGOOSE/blob/main/instruct_goose/agent.py#L112){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### AgentObjective.forward\n",
       "\n",
       ">      AgentObjective.forward (input_ids:typing.Annotated[torch.Tensor,{'__torch\n",
       ">                              typing__':True,'details':('batch_size','seq_len',\n",
       ">                              ),'cls_name':'TensorType'}], attention_mask:typin\n",
       ">                              g.Annotated[torch.Tensor,{'__torchtyping__':True,\n",
       ">                              'details':('batch_size','seq_len',),'cls_name':'T\n",
       ">                              ensorType'}])\n",
       "\n",
       "Calculate the objective value given the input ids and attention mask.\n",
       "\n",
       "|    | **Type** | **Details** |\n",
       "| -- | -------- | ----------- |\n",
       "| input_ids | Annotated |  |\n",
       "| attention_mask | Annotated |  |\n",
       "| **Returns** | **Annotated** | **A scalar objective value** |"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(AgentObjective.forward)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
