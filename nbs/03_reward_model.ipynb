{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reward Model\n",
    "\n",
    "> Reward Model and Pairwise Loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import torch\n",
    "from torch import nn\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "from torchtyping import TensorType"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reward Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class RewardModel(nn.Module):\n",
    "    \"\"\"Reward model.\"\"\"\n",
    "    def __init__(\n",
    "        self, checkpoint: str, # `transformers`'s model path\n",
    "        dropout: float = 0.1 \n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.model = AutoModel.from_pretrained(checkpoint)\n",
    "        \n",
    "        config = self.model.config\n",
    "        n_embed = config.n_embd\n",
    "        \n",
    "        # custom head\n",
    "        self.reward_head = nn.Sequential(\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(n_embed, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: TensorType[\"batch_size\", \"seq_len\"],\n",
    "        attention_mask: TensorType[\"batch_size\", \"seq_len\"],\n",
    "    ) -> TensorType[\"batch_size\", 1]: # A reward scalar for each item in a batch\n",
    "        \"\"\"Calculate reward for each item in a batch.\"\"\"\n",
    "        last_hidden_state = self.model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "        ).last_hidden_state\n",
    "        \n",
    "        output = self.reward_head(last_hidden_state)\n",
    "                \n",
    "        # for eacb item in the batch\n",
    "        # choose the hidden state of the last token as a reward!\n",
    "        reward_scalar = output[:, -1, 0]\n",
    "        \n",
    "        return reward_scalar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "### RewardModel\n",
       "\n",
       ">      RewardModel (checkpoint:str, dropout:float=0.1)\n",
       "\n",
       "Reward model.\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| checkpoint | str |  | `transformers`'s model path |\n",
       "| dropout | float | 0.1 |  |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "### RewardModel\n",
       "\n",
       ">      RewardModel (checkpoint:str, dropout:float=0.1)\n",
       "\n",
       "Reward model.\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| checkpoint | str |  | `transformers`'s model path |\n",
       "| dropout | float | 0.1 |  |"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(RewardModel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "### RewardModel.forward\n",
       "\n",
       ">      RewardModel.forward (input_ids:typing.Annotated[torch.Tensor,{'__torchtyp\n",
       ">                           ing__':True,'details':('batch_size','seq_len',),'cls\n",
       ">                           _name':'TensorType'}], attention_mask:typing.Annotat\n",
       ">                           ed[torch.Tensor,{'__torchtyping__':True,'details':('\n",
       ">                           batch_size','seq_len',),'cls_name':'TensorType'}])\n",
       "\n",
       "Calculate reward for each item in a batch.\n",
       "\n",
       "|    | **Type** | **Details** |\n",
       "| -- | -------- | ----------- |\n",
       "| input_ids | Annotated |  |\n",
       "| attention_mask | Annotated |  |\n",
       "| **Returns** | **Annotated** | **A reward scalar for each item in a batch** |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "### RewardModel.forward\n",
       "\n",
       ">      RewardModel.forward (input_ids:typing.Annotated[torch.Tensor,{'__torchtyp\n",
       ">                           ing__':True,'details':('batch_size','seq_len',),'cls\n",
       ">                           _name':'TensorType'}], attention_mask:typing.Annotat\n",
       ">                           ed[torch.Tensor,{'__torchtyping__':True,'details':('\n",
       ">                           batch_size','seq_len',),'cls_name':'TensorType'}])\n",
       "\n",
       "Calculate reward for each item in a batch.\n",
       "\n",
       "|    | **Type** | **Details** |\n",
       "| -- | -------- | ----------- |\n",
       "| input_ids | Annotated |  |\n",
       "| attention_mask | Annotated |  |\n",
       "| **Returns** | **Annotated** | **A reward scalar for each item in a batch** |"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(RewardModel.forward)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pairwise Loss\n",
    "\n",
    "$\\operatorname{loss}(\\theta)=-\\frac{1}{\\left(\\begin{array}{c}\n",
    "K \\\\\n",
    "2\n",
    "\\end{array}\\right)} E_{\\left(x, y_w, y_l\\right) \\sim D}\\left[\\log \\left(\\sigma\\left(r_\\theta\\left(x, y_w\\right)-r_\\theta\\left(x, y_l\\right)\\right)\\right)\\right]$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class PairwiseLoss(nn.Module):\n",
    "    \"\"\"Pairwise loss function.\"\"\"\n",
    "    def forward(\n",
    "        self,\n",
    "        chosen_rewards: TensorType[\"batch_size\", 1], # The reward of the chosen prompt\n",
    "        rejected_rewards: TensorType[\"batch_size\", 1] # The reward of the rejected prompt\n",
    "    ) -> TensorType[1]: # A scalar loss\n",
    "        \"\"\"Forward pass.\"\"\"\n",
    "        assert len(chosen_rewards) == len(rejected_rewards)\n",
    "        batch_size = len(chosen_rewards)\n",
    "        \n",
    "        # maps the difference between the rewards to a probability\n",
    "        probs = torch.sigmoid(chosen_rewards - rejected_rewards)\n",
    "        return -probs.mean() / batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "### PairwiseLoss.forward\n",
       "\n",
       ">      PairwiseLoss.forward (chosen_rewards:typing.Annotated[torch.Tensor,{'__to\n",
       ">                            rchtyping__':True,'details':('batch_size',1,),'cls_\n",
       ">                            name':'TensorType'}], rejected_rewards:typing.Annot\n",
       ">                            ated[torch.Tensor,{'__torchtyping__':True,'details'\n",
       ">                            :('batch_size',1,),'cls_name':'TensorType'}])\n",
       "\n",
       "Forward pass.\n",
       "\n",
       "|    | **Type** | **Details** |\n",
       "| -- | -------- | ----------- |\n",
       "| chosen_rewards | Annotated | The reward of the chosen prompt |\n",
       "| rejected_rewards | Annotated | The reward of the rejected prompt |\n",
       "| **Returns** | **Annotated** | **A scalar loss** |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "### PairwiseLoss.forward\n",
       "\n",
       ">      PairwiseLoss.forward (chosen_rewards:typing.Annotated[torch.Tensor,{'__to\n",
       ">                            rchtyping__':True,'details':('batch_size',1,),'cls_\n",
       ">                            name':'TensorType'}], rejected_rewards:typing.Annot\n",
       ">                            ated[torch.Tensor,{'__torchtyping__':True,'details'\n",
       ">                            :('batch_size',1,),'cls_name':'TensorType'}])\n",
       "\n",
       "Forward pass.\n",
       "\n",
       "|    | **Type** | **Details** |\n",
       "| -- | -------- | ----------- |\n",
       "| chosen_rewards | Annotated | The reward of the chosen prompt |\n",
       "| rejected_rewards | Annotated | The reward of the rejected prompt |\n",
       "| **Returns** | **Annotated** | **A scalar loss** |"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(PairwiseLoss.forward)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
