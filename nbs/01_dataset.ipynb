{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset\n",
    "\n",
    "> Dataset for RL-based model and Reward model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from typing import Callable, Tuple, Iterable\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "from tqdm import tqdm\n",
    "from torchtyping import TensorType"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset for Reward Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class PairDataset(Dataset):\n",
    "    \"\"\"Pairwise dataset for train reward model.\"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        dataset: Iterable, # A dataset\n",
    "        tokenizer: Callable, # The tokenizer of the reward model\n",
    "        max_length: int = 1024 # Max context length of the reward model\n",
    "    ):\n",
    "        \n",
    "        self.chosen = []\n",
    "        self.rejected = []\n",
    "        \n",
    "        for data in tqdm(dataset):\n",
    "            chosen, rejected = data[\"chosen\"], data[\"rejected\"]\n",
    "            chosen_encoding = tokenizer(\n",
    "                chosen,\n",
    "                max_length=max_length, padding=\"max_length\", truncation=True,\n",
    "                return_tensors=\"pt\"\n",
    "            )\n",
    "            rejected_encoding = tokenizer(\n",
    "                rejected,\n",
    "                max_length=max_length, padding=\"max_length\", truncation=True,\n",
    "                return_tensors=\"pt\"\n",
    "            )\n",
    "            \n",
    "            self.chosen.append({\n",
    "                \"input_ids\": chosen_encoding[\"input_ids\"],\n",
    "                \"attention_mask\": chosen_encoding[\"attention_mask\"]\n",
    "            })\n",
    "            self.rejected.append({\n",
    "                \"input_ids\": rejected_encoding[\"input_ids\"],\n",
    "                \"attention_mask\": rejected_encoding[\"attention_mask\"]\n",
    "            })\n",
    "    \n",
    "    def __len__(self) -> int:\n",
    "        return len(self.chosen)\n",
    "\n",
    "    def __getitem__(self, idx: int) -> Tuple[\n",
    "        TensorType[\"seq_len\"], TensorType[\"seq_len\"],\n",
    "        TensorType[\"seq_len\"], TensorType[\"seq_len\"]\n",
    "    ]:\n",
    "        return self.chosen[idx][\"input_ids\"],\\\n",
    "               self.chosen[idx][\"attention_mask\"],\\\n",
    "               self.rejected[idx][\"input_ids\"],\\\n",
    "               self.rejected[idx][\"attention_mask\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "### PairDataset\n",
       "\n",
       ">      PairDataset (dataset:Iterable, tokenizer:Callable, max_length:int=1024)\n",
       "\n",
       "Pairwise dataset for train reward model.\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| dataset | Iterable |  | A dataset |\n",
       "| tokenizer | Callable |  | The tokenizer of the reward model |\n",
       "| max_length | int | 1024 | Max context length of the reward model |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "### PairDataset\n",
       "\n",
       ">      PairDataset (dataset:Iterable, tokenizer:Callable, max_length:int=1024)\n",
       "\n",
       "Pairwise dataset for train reward model.\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| dataset | Iterable |  | A dataset |\n",
       "| tokenizer | Callable |  | The tokenizer of the reward model |\n",
       "| max_length | int | 1024 | Max context length of the reward model |"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(PairDataset)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset for PPO Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class PromptDataset(Dataset):\n",
    "    \"\"\"Dataset for train RL-based language model.\"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        dataset: Iterable, # A dataset\n",
    "        tokenizer: Callable, # The tokenizer of the language model\n",
    "        max_length: int = 1024 # Max context length of the language model\n",
    "    ):\n",
    "        self.prompts = []\n",
    "        \n",
    "        for data in tqdm(dataset):\n",
    "            prompt = data[\"prompt\"]\n",
    "            prompt_encoding = tokenizer(\n",
    "                prompt,\n",
    "                max_length=max_length, padding=\"max_length\", truncation=True,\n",
    "                return_tensors=\"pt\"\n",
    "            )\n",
    "            \n",
    "            self.prompts.append({\n",
    "                \"input_ids\": prompt_encoding[\"input_ids\"],\n",
    "                \"attention_mask\": prompt_encoding[\"attention_mask\"]\n",
    "            })\n",
    "            \n",
    "    def __len__(self):\n",
    "        return len(self.prompts)\n",
    "\n",
    "    def __getitem__(self, idx: int) -> Tuple[TensorType[\"seq_len\"], TensorType[\"seq_len\"]]:\n",
    "        return self.prompts[idx][\"input_ids\"],\\\n",
    "               self.prompts[idx][\"attention_mask\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "### PromptDataset\n",
       "\n",
       ">      PromptDataset (dataset:Iterable, tokenizer:Callable, max_length:int=1024)\n",
       "\n",
       "Dataset for train RL-based language model.\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| dataset | Iterable |  | A dataset |\n",
       "| tokenizer | Callable |  | The tokenizer of the language model |\n",
       "| max_length | int | 1024 | Max context length of the language model |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "### PromptDataset\n",
       "\n",
       ">      PromptDataset (dataset:Iterable, tokenizer:Callable, max_length:int=1024)\n",
       "\n",
       "Dataset for train RL-based language model.\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| dataset | Iterable |  | A dataset |\n",
       "| tokenizer | Callable |  | The tokenizer of the language model |\n",
       "| max_length | int | 1024 | Max context length of the language model |"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(PromptDataset)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
