{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# InstructGoose - ðŸš§ WORK IN PROGRESS ðŸš§\n",
    "\n",
    "> Implementation of Reinforcement Learning from Human Feedback (RLHF) from the InstructGPT paper"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Paper: InstructGPT - [Training language models to follow instructions with human feedback](https://arxiv.org/abs/2203.02155)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install from PipPy"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```sh\n",
    "pip install instruct-goose\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install directly from the source code"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```sh\n",
    "git clone https://github.com/xrsrke/instructGOOSE.git\n",
    "cd instructGOOSE\n",
    "pip install -e .\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the RL-based language model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from datasets import load_dataset\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch import optim\n",
    "\n",
    "from instruct_goose import Agent, RewardModel, RLHFTrainer, RLHFConfig, create_reference_model\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 1:** Load dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "dataset = load_dataset(\"imdb\", split=\"train\")\n",
    "train_dataloader = DataLoader(dataset, batch_size=4, shuffle=True)\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 2**: Load the pre-trained model and tokenizer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "model_base = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
    "reward_model = RewardModel(\"gpt2\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "eos_token_id = tokenizer.eos_token_id\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 3**: Create the RL-based language model agent and the reference model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "model = Agent(model_base)\n",
    "ref_model = create_reference_model(model)\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 4**: Train it"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "max_new_tokens = 20\n",
    "generation_kwargs = {\n",
    "    \"min_length\":-1,\n",
    "    \"top_k\": 0.0,\n",
    "    \"top_p\": 1.0,\n",
    "    \"do_sample\": True,\n",
    "    \"pad_token_id\": tokenizer.eos_token_id,\n",
    "    \"max_new_tokens\": max_new_tokens\n",
    "}\n",
    "\n",
    "config = RLHFConfig()\n",
    "N_EPOCH = 100\n",
    "trainer = RLHFTrainer(model, ref_model, config)\n",
    "optimizer = optim.SGD(model.parameters(), lr=1e-3)\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "for epoch in range(N_EPOCH):\n",
    "    for batch in train_dataloader:\n",
    "        inputs = tokenizer(batch[\"text\"], padding=True, truncation=True, return_tensors=\"pt\")\n",
    "        responses = model.generate(\n",
    "            inputs[\"input_ids\"], attention_mask=inputs[\"attention_mask\"],\n",
    "            **generation_kwargs\n",
    "        )\n",
    "        \n",
    "        # extract the generated text\n",
    "        responses = responses[:, -max_new_tokens:]\n",
    "        \n",
    "        # evaluate from the reward model\n",
    "        with torch.no_grad():\n",
    "            text_input_ids = torch.stack([torch.concat([q, r]) for q, r in zip(inputs[\"input_ids\"], responses)], dim=0)\n",
    "            texts = tokenizer.batch_decode(text_input_ids, skip_special_tokens=True)\n",
    "            rewards = reward_model(texts)\n",
    "        \n",
    "        # calculate PPO loss\n",
    "        loss = trainer.compute_loss(inputs[\"input_ids\"], responses, rewards)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        print(f\"loss={loss}\")\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO\n",
    "\n",
    "- Add support custom reward function\n",
    "- Add support custom value function\n",
    "- Add support non-transformer models"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questions\n",
    "- In the context of RLHF, how to calculate the $L_t^{V F}(\\theta)$, \n",
    "    + Like it's a function of the PPO agent uses to predict how much reward it gets if generates the sequence?\n",
    "- ~~Does the RL model and the SFT model use the same tokenizer?\n",
    "    Yes~~\n",
    "- ~~I don't know how to returns the logit of the generation model~~\n",
    "- Does the PPO Agent (Language Model) has a value network just like the regular PPO Agent?\n",
    "- I don't understand how to calculate the advantage in PPO"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resources\n",
    "I used these resources to implement this\n",
    "\n",
    "- Copied the `load_yaml` function from https://github.com/Dahoas/reward-modeling\n",
    "- How to build a dataset to train reward model: https://wandb.ai/carperai/summarize_RLHF/reports/Implementing-RLHF-Learning-to-Summarize-with-trlX--VmlldzozMzAwODM2\n",
    "- How to add value head in PPO agent: https://github.com/lvwerra/trl\n",
    "- How to calculate the loss of PPO agent: https://github.com/lvwerra/trl/blob/main/trl/trainer/ppo_trainer.py\n",
    "- How to use PPO to train RLHF agent: https://github.com/voidful/TextRL\n",
    "- How PPO works: https://github.com/vwxyzjn/cleanrl/blob/master/cleanrl/ppo.py\n",
    "- Copied the compute `advantages` and `returns` from `TLR`: https://github.com/lvwerra/trl/blob/d2e8bcf8373726fb92d2110c500f7df6d0bd566d/trl/trainer/ppo_trainer.py#L686"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
